<div id="glossary-content" class="space-y-4 text-sm max-h-[80vh] overflow-y-auto pr-2">
    <div id="alternative-hypothesis"><h4 class="font-semibold text-base">Alternative Hypothesis ($H_1$)</h4><p>Your research hypothesis; the claim that there <em>is</em> an effect, a difference, or a relationship.</p></div>
    <div id="anova-glossary"><h4 class="font-semibold text-base">ANOVA (Analysis of Variance)</h4><p>A statistical test used to compare the means of three or more groups.</p></div>
    <div id="between-subjects"><h4 class="font-semibold text-base">Between-Subjects Design</h4><p>An experimental design where different participants are assigned to each condition or group.</p></div>
    <div id="bonferroni-correction"><h4 class="font-semibold text-base">Bonferroni Correction</h4><p>A method used in post-hoc tests to counteract the problem of inflated Type I error that occurs when performing multiple statistical comparisons. It divides the alpha level (e.g., .05) by the number of tests being run.</p></div>
    <div id="coefficient-of-determination"><h4 class="font-semibold text-base">Coefficient of Determination ($R^2$)</h4><p>A value in regression and correlation that represents the proportion of variance in the dependent variable that can be predicted from the independent variable(s). For example, an RÂ² of .25 means 25% of the outcome's variability is explained by the model.</p></div>
    <div id="correlation-glossary"><h4 class="font-semibold text-base">Correlation</h4><p>A measure of the extent to which two continuous variables are related.</p></div>
    <div id="degrees-of-freedom"><h4 class="font-semibold text-base">Degrees of Freedom (df)</h4><p>The number of values in a final calculation that are free to vary. It's related to sample size and is used to determine the critical value for a statistical test.</p></div>
    <div id="dv"><h4 class="font-semibold text-base">Dependent Variable (DV)</h4><p>The variable that is measured by the experimenter. It is the 'outcome' variable that you expect to be influenced by the independent variable.</p></div>
    <div id="effect-size-glossary"><h4 class="font-semibold text-base">Effect Size</h4><p>A measure of the magnitude or size of a statistical effect, independent of sample size. It tells you how meaningful the effect is. Examples include Cohen's <em>d</em> and partial eta squared ($\eta_p^2$).</p></div>
    <div id="f-ratio"><h4 class="font-semibold text-base">F-Ratio</h4><p>The test statistic calculated in an ANOVA. It is the ratio of the variance between groups (signal) to the variance within groups (noise).</p></div>
    <div id="factorial-anova-glossary"><h4 class="font-semibold text-base">Factorial ANOVA</h4><p>An ANOVA that includes two or more independent variables (factors), allowing for the testing of main effects and interaction effects.</p></div>
    <div id="greenhouse-geisser"><h4 class="font-semibold text-base">Greenhouse-Geisser Correction</h4><p>An adjustment made to the degrees of freedom in a repeated-measures ANOVA when the assumption of sphericity has been violated. It's a conservative correction.</p></div>
    <div id="homogeneity-of-variance"><h4 class="font-semibold text-base">Homogeneity of Variance</h4><p>An assumption of the independent-samples t-test and ANOVA that the variance in scores is the same for each group.</p></div>
    <div id="independent-samples-t-test"><h4 class="font-semibold text-base">Independent-Samples t-Test</h4><p>A test used to compare the means of two groups when each group is made up of different participants.</p></div>
    <div id="interaction-effect"><h4 class="font-semibold text-base">Interaction Effect</h4><p>In a factorial ANOVA, this occurs when the effect of one independent variable on the dependent variable depends on the level of another independent variable.</p></div>
    <div id="intercept"><h4 class="font-semibold text-base">Intercept ($b_0$)</h4><p>In regression, this is the predicted value of the outcome variable (Y) when the predictor variable (X) is equal to zero. It's the point where the regression line crosses the y-axis.</p></div>
    <div id="iv"><h4 class="font-semibold text-base">Independent Variable (IV)</h4><p>The variable that is manipulated or changed by the experimenter to see if it has an effect on the dependent variable.</p></div>
    <div id="levenes-test"><h4 class="font-semibold text-base">Levene's Test</h4><p>A test used to check the assumption of homogeneity of variance. A non-significant result (p > .05) indicates the assumption is met.</p></div>
    <div id="line-of-best-fit"><h4 class="font-semibold text-base">Line of Best Fit</h4><p>The line on a scatterplot that comes closest to all of the dots in the plot, used in regression to make predictions.</p></div>
    <div id="main-effect"><h4 class="font-semibold text-base">Main Effect</h4><p>In a factorial ANOVA, this is the overall effect of one independent variable on its own, ignoring the other independent variables.</p></div>
    <div id="mauchlys-test"><h4 class="font-semibold text-base">Mauchly's Test</h4><p>A test used to check the assumption of sphericity in repeated-measures ANOVAs. A significant result (p < .05) indicates the assumption has been violated.</p></div>
    <div id="mean"><h4 class="font-semibold text-base">Mean ($\bar{x}$)</h4><p>The arithmetic average of a set of scores, calculated by adding all scores together and dividing by the number of scores.</p></div>
    <div id="mixed-anova"><h4 class="font-semibold text-base">Mixed ANOVA</h4><p>An ANOVA that combines between-subjects and within-subjects factors in the same design.</p></div>
    <div id="multiple-regression"><h4 class="font-semibold text-base">Multiple Regression</h4><p>A statistical technique that uses two or more predictor variables to predict a single outcome variable.</p></div>
    <div id="multicollinearity"><h4 class="font-semibold text-base">Multicollinearity</h4><p>A problem in multiple regression where two or more predictor variables are very highly correlated with each other, making it difficult to determine their individual effects.</p></div>
    <div id="null-hypothesis"><h4 class="font-semibold text-base">Null Hypothesis ($H_0$)</h4><p>The default assumption that there is no effect, no difference, or no relationship. The goal of a statistical test is to see if there is enough evidence to reject this.</p></div>
    <div id="p-value"><h4 class="font-semibold text-base">p-value</h4><p>The probability of observing your data (or more extreme data) if the null hypothesis were true. A small p-value (typically < .05) suggests your result is unlikely to be due to chance.</p></div>
    <div id="paired-samples-t-test"><h4 class="font-semibold text-base">Paired-Samples t-Test</h4><p>A test used to compare the means of two conditions when the same participants took part in both conditions.</p></div>
    <div id="pearsons-r"><h4 class="font-semibold text-base">Pearson's r</h4><p>The correlation coefficient, a value between -1 and +1 that indicates the strength and direction of a linear relationship between two variables.</p></div>
    <div id="planned-comparisons"><h4 class="font-semibold text-base">Planned Comparisons</h4><p>Follow-up tests to an ANOVA that are used when you have specific, theory-driven hypotheses about which groups will differ.</p></div>
    <div id="post-hoc"><h4 class="font-semibold text-base">Post-Hoc Tests</h4><p>Follow-up tests to an ANOVA used to compare every group with every other group to find where significant differences lie. They often include a correction to control for errors.</p></div>
    <div id="power"><h4 class="font-semibold text-base">Power</h4><p>The probability of correctly rejecting the null hypothesis when it is false. In simple terms, it's the chance of finding a real effect if one exists. The target is usually .80 (80%).</p></div>
    <div id="regression"><h4 class="font-semibold text-base">Regression</h4><p>A statistical technique used to model and predict the relationship between a dependent (outcome) variable and one or more independent (predictor) variables.</p></div>
    <div id="scatterplot"><h4 class="font-semibold text-base">Scatterplot</h4><p>A graph used to visually represent the relationship between two continuous variables, where each dot represents a pair of scores.</p></div>
    <div id="simple-effects"><h4 class="font-semibold text-base">Simple Effects</h4><p>An analysis used to break down a significant interaction. It looks at the effect of one independent variable at each individual level of the other independent variable.</p></div>
    <div id="slope"><h4 class="font-semibold text-base">Slope ($b_1$)</h4><p>In regression, this value represents the change in the outcome variable (Y) for a one-unit increase in the predictor variable (X). It determines the steepness of the regression line.</p></div>
    <div id="sphericity"><h4 class="font-semibold text-base">Sphericity</h4><p>An assumption for repeated-measures ANOVAs with 3+ levels. It assumes the variances of the differences between all possible pairs of conditions are equal. Tested with Mauchly's Test.</p></div>
    <div id="standard-deviation"><h4 class="font-semibold text-base">Standard Deviation (SD)</h4><p>A measure of the average amount of variability or spread in a set of scores. It is the square root of the variance.</p></div>
    <div id="statistical-significance"><h4 class="font-semibold text-base">Statistical Significance</h4><p>A result is statistically significant if it is unlikely to have occurred by random chance alone. In psychology, the threshold is typically a p-value less than .05.</p></div>
    <div id="sum-of-squares"><h4 class="font-semibold text-base">Sum of Squares (SS)</h4><p>A fundamental concept in ANOVA; it's the sum of the squared differences of each score from the mean. It's a measure of total variability.</p></div>
    <div id="systematic-variance"><h4 class="font-semibold text-base">Systematic Variance</h4><p>The variability in data that is attributable to the experimenter's manipulation (the independent variable). It's the 'signal' in an ANOVA.</p></div>
    <div id="t-test"><h4 class="font-semibold text-base">t-Test</h4><p>A statistical test used to compare the means of exactly two groups.</p></div>
    <div id="type-i-error"><h4 class="font-semibold text-base">Type I Error</h4><p>A "false positive." Rejecting the null hypothesis when it was actually true.</p></div>
    <div id="type-ii-error"><h4 class="font-semibold text-base">Type II Error</h4><p>A "false negative." Failing to reject the null hypothesis when it was actually false.</p></div>
    <div id="unsystematic-variance"><h4 class="font-semibold text-base">Unsystematic Variance</h4><p>The variability in data that is due to random, unpredictable factors like individual differences or measurement error. It's the 'noise' in an ANOVA.</p></div>
    <div id="variance"><h4 class="font-semibold text-base">Variance</h4><p>A measure of the spread or dispersion of a set of scores. It's the average of the squared deviations from the mean.</p></div>
    <div id="within-subjects"><h4 class="font-semibold text-base">Within-Subjects Design (Repeated Measures)</h4><p>An experimental design where the same participants take part in all conditions.</p></div>
</div>
