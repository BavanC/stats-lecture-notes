<div id="glossary-content" class="space-y-4 text-sm max-h-[80vh] overflow-y-auto pr-2">
    <div id="alternative-hypothesis"><h4 class="font-semibold text-base">Alternative Hypothesis ($H_1$)</h4><p>Your research hypothesis; the claim that there <em>is</em> an effect, a difference, or a relationship.</p></div>
    <div id="anova-glossary"><h4 class="font-semibold text-base">ANOVA (Analysis of Variance)</h4><p>A statistical test used to compare the means of three or more groups.</p></div>
    <div id="between-subjects"><h4 class="font-semibold text-base">Between-Subjects Design</h4><p>An experimental design where different participants are assigned to each condition or group.</p></div>
    <div id="bonferroni-correction"><h4 class="font-semibold text-base">Bonferroni Correction</h4><p>A method used in post-hoc tests to counteract the problem of inflated Type I error that occurs when performing multiple statistical comparisons. It divides the alpha level (e.g., .05) by the number of tests being run.</p></div>
    <div id="coefficient-of-determination"><h4 class="font-semibold text-base">Coefficient of Determination ($R^2$)</h4><p>A value in regression and correlation that represents the proportion of variance in the dependent variable that can be predicted from the independent variable(s). For example, an RÂ² of .25 means 25% of the outcome's variability is explained by the model.</p></div>
    <div id="correlation-glossary"><h4 class="font-semibold text-base">Correlation</h4><p>A measure of the extent to which two continuous variables are related.</p></div>
    <div id="degrees-of-freedom"><h4 class="font-semibold text-base">Degrees of Freedom (df)</h4><p>The number of values in a final calculation that are free to vary. It's related to sample size and is used to determine the critical value for a statistical test.</p></div>
    <div id="dv"><h4 class="font-semibold text-base">Dependent Variable (DV)</h4><p>The variable that is measured by the experimenter. It is the 'outcome' variable that you expect to be influenced by the independent variable.</p></div>
    <div id="effect-size-glossary"><h4 class="font-semibold text-base">Effect Size</h4><p>A measure of the magnitude or size of a statistical effect, independent of sample size. It tells you how meaningful the effect is. Examples include Cohen's <em>d</em> and partial eta squared ($\eta_p^2$).</p></div>
    <div id="f-ratio"><h4 class="font-semibold text-base">F-Ratio</h4><p>The test statistic calculated in an ANOVA. It is the ratio of the variance between groups (signal) to the variance within groups (noise).</p></div>
    <div id="factorial-anova-glossary"><h4 class="font-semibold text-base">Factorial ANOVA</h4><p>An ANOVA that includes two or more independent variables (factors), allowing for the testing of main effects and interaction effects.</p></div>
    <div id="greenhouse-geisser"><h4 class="font-semibold text-base">Greenhouse-Geisser Correction</h4><p>An adjustment made to the degrees of freedom in a repeated-measures ANOVA when the assumption of sphericity has been violated. It's a conservative correction.</p></div>
    <div id="homogeneity-of-variance"><h4 class="font-semibold text-base">Homogeneity of Variance</h4><p>An assumption of the independent-samples t-test and ANOVA that the variance in scores is the same for each group.</p></div>
    <div id="independent-samples-t-test"><h4 class="font-semibold text-base">Independent-Samples t-Test</h4><p>A test used to compare the means of two groups when each group is made up of different participants.</p></div>
    <div id="interaction-effect"><h4 class="font-semibold text-base">Interaction Effect</h4><p>In a factorial ANOVA, this occurs when the effect of one independent variable on the dependent variable depends on the level of another independent variable.</p></div>
    <div id="intercept"><h4 class="font-semibold text-base">Intercept ($b_0$)</h4><p>In regression, this is the predicted value of the outcome variable (Y) when the predictor variable (X) is equal to zero. It's the point where the regression line crosses the y-axis.</p></div>
    <div id="iv"><h4 class="font-semibold text-base">Independent Variable (IV)</h4><p>The variable that is manipulated or changed by the experimenter to see if it has an effect on the dependent variable.</p></div>
    <div id="levenes-test"><h4 class="font-semibold text-base">Levene's Test</h4><p>A test used to check the assumption of homogeneity of variance. A non-significant result (p > .05) indicates the assumption is met.</p></div>
    <div id="line-of-best-fit"><h4 class="font-semibold text-base">Line of Best Fit</h4><p>The line on a scatterplot that comes closest to all of the dots in the plot, used in regression to make predictions.</p></div>
    <div id="main-effect"><h4 class="font-semibold text-base">Main Effect</h4><p>In a factorial ANOVA, this is the overall effect of one independent variable on its own, ignoring the other independent variables.</p></div>
    <div id="mauchlys-test"><h4 class="font-semibold text-base">Mauchly's Test</h4><p>A test used to check the assumption of sphericity in repeated-measures ANOVAs. A significant result (p < .05) indicates the assumption has been violated.</p></div>
    <div id="mean"><h4 class="font-semibold text-base">Mean ($\bar{x}$)</h4><p>The arithmetic average of a set of scores, calculated by adding all scores together and dividing by the number of scores.</p></div>
    <div id="mixed-anova"><h4 class="font-semibold text-base">Mixed ANOVA</h4><p>An ANOVA that combines between-subjects and within-subjects factors in the same design.</p></div>
    <div id="multiple-regression"><h4 class="font-semibold text-base">Multiple Regression</h4><p>A statistical technique that uses two or more predictor variables to predict a single outcome variable.</p></div>
    <div id="multicollinearity"><h4 class="font-semibold text-base">Multicollinearity</h4><p>A problem in multiple regression where two or more predictor variables are very highly correlated with each other, making it difficult to determine their individual effects.</p></div>
    <div id="null-hypothesis"><h4 class="font-semibold text-base">Null Hypothesis ($H_0$)</h4><p>The default assumption that there is no effect, no difference, or no relationship. The goal of a statistical test is to see if there is enough evidence to reject this.</p></div>
    <div id="p-value"><h4 class="font-semibold text-base">p-value</h4><p>The probability of observing your data (or more extreme data) if the null hypothesis were true. A small p-value (typically < .05) suggests your result is unlikely to be due to chance.</p></div>
    <div id="paired-samples-t-test"><h4 class="font-semibold text-base">Paired-Samples t-Test</h4><p>A test used to compare the means of two conditions when the same participants took part in both conditions.</p></div>
    <div id="pearsons-r"><h4 class="font-semibold text-base">Pearson's r</h4><p>The correlation coefficient, a value between -1 and +1 that indicates the strength and direction of a linear relationship between two variables.</p></div>
    <div id="planned-comparisons"><h4 class="font-semibold text-base">Planned Comparisons</h4><p>Follow-up tests to an ANOVA that are used when you have specific, theory-driven hypotheses about which groups will differ.</p></div>
    <div id="post-hoc"><h4 class="font-semibold text-base">Post-Hoc Tests</h4><p>Follow-up tests to an ANOVA used to compare every group with every other group to find where significant differences lie. They often include a correction to control for errors.</p></div>
    <div id="power"><h4 class="font-semibold text-base">Power</h4><p>The probability of correctly rejecting the null hypothesis when it is false. In simple terms, it's the chance of finding a real effect if one exists. The target is usually .80 (80%).</p></div>
    <div id="regression"><h4 class="font-semibold text-base">Regression</h4><p>A statistical technique used to model and predict the relationship between a dependent (outcome) variable and one or more independent (predictor) variables.</p></div>
    <div id="scatterplot"><h4 class="font-semibold text-base">Scatterplot</h4><p>A graph used to visually represent the relationship between two continuous variables, where each dot represents a pair of scores.</p></div>
    <div id="simple-effects"><h4 class="font-semibold text-base">Simple Effects</h4><p>An analysis used to break down a significant interaction. It looks at the effect of one independent variable at each individual level of the other independent variable.</p></div>
    <div id="slope"><h4 class="font-semibold text-base">Slope ($b_1$)</h4><p>In regression, this value represents the change in the outcome variable (Y) for a one-unit increase in the predictor variable (X). It determines the steepness of the regression line.</p></div>
    <div id="sphericity"><h4 class="font-semibold text-base">Sphericity</h4><p>An assumption for repeated-measures ANOVAs with 3+ levels. It assumes the variances of the differences between all possible pairs of conditions are equal. Tested with Mauchly's Test.</p></div>
    <div id="standard-deviation"><h4 class="font-semibold text-base">Standard Deviation (SD)</h4><p>A measure of the average amount of variability or spread in a set of scores. It is the square root of the variance.</p></div>
    <div id="statistical-significance"><h4 class="font-semibold text-base">Statistical Significance</h4><p>A result is statistically significant if it is unlikely to have occurred by random chance alone. In psychology, the threshold is typically a p-value less than .05.</p></div>
    <div id="sum-of-squares"><h4 class="font-semibold text-base">Sum of Squares (SS)</h4><p>A fundamental concept in ANOVA; it's the sum of the squared differences of each score from the mean. It's a measure of total variability.</p></div>
    <div id="systematic-variance"><h4 class="font-semibold text-base">Systematic Variance</h4><p>The variability in data that is attributable to the experimenter's manipulation (the independent variable). It's the 'signal' in an ANOVA.</p></div>
    <div id="t-test"><h4 class="font-semibold text-base">t-Test</h4><p>A statistical test used to compare the means of exactly two groups.</p></div>
    <div id="type-i-error"><h4 class="font-semibold text-base">Type I Error</h4><p>A "false positive." Rejecting the null hypothesis when it was actually true.</p></div>
    <div id="type-ii-error"><h4 class="font-semibold text-base">Type II Error</h4><p>A "false negative." Failing to reject the null hypothesis when it was actually false.</p></div>
    <div id="unsystematic-variance"><h4 class="font-semibold text-base">Unsystematic Variance</h4><p>The variability in data that is due to random, unpredictable factors like individual differences or measurement error. It's the 'noise' in an ANOVA.</p></div>
    <div id="variance"><h4 class="font-semibold text-base">Variance</h4><p>A measure of the spread or dispersion of a set of scores. It's the average of the squared deviations from the mean.</p></div>
    <div id="within-subjects"><h4 class="font-semibold text-base">Within-Subjects Design (Repeated Measures)</h4><p>An experimental design where the same participants take part in all conditions.</p></div>
    <div id="adjusted-r-squared"><h4 class="font-semibold text-base">Adjusted RÂ²</h4><p>An estimate of the coefficient of determination (RÂ²) in multiple regression that accounts for the number of predictors in the model. It helps to estimate how well your model would generalize to the wider population.</p></div>
    <div id="bartletts-test"><h4 class="font-semibold text-base">Bartlett's Test of Sphericity</h4><p>An assumption check used before running an Exploratory Factor Analysis (EFA). It tests whether there are sufficient correlations between your variables to proceed with the analysis. A significant result (p < .05) is required.</p></div>
    <div id="binary-logistic-regression"><h4 class="font-semibold text-base">Binary Logistic Regression</h4><p>A type of regression used when your outcome (dependent) variable is dichotomousâthat is, it has only two possible outcomes (e.g., Pass/Fail, Yes/No).</p></div>
    <div id="categorical-data"><h4 class="font-semibold text-base">Categorical Data</h4><p>Data that consists of counts or frequencies within different categories, rather than continuous measurements. Examples include gender, newspaper choice, or degree classification.</p></div>
    <div id="censoring"><h4 class="font-semibold text-base">Censoring / Censored Data</h4><p>A key concept in survival analysis. A participant's data is 'censored' if the event of interest (e.g., recovery, relapse) has not occurred by the end of the study, or if they drop out. Survival analysis is specifically designed to handle this information.</p></div>
    <div id="chi-square-test"><h4 class="font-semibold text-base">Chi-Square (ÏÂ²) Test</h4><p>A statistical test used to analyze categorical data. It can be used to see if the observed frequencies in categories differ from expected frequencies (Goodness of Fit) or if there is an association between two categorical variables (Test of Association).</p></div>
    <div id="confusion-matrix"><h4 class="font-semibold text-base">Confusion Matrix</h4><p>A table used in classification models, like binary logistic regression, to assess performance. It shows the number of correct and incorrect predictions made by the model (e.g., true positives, true negatives, false positives, and false negatives).</p></div>
    <div id="cramers-v"><h4 class="font-semibold text-base">Cramer's V</h4><p>An effect size measure used for a Chi-Square test of association. It measures the strength of the relationship between two categorical variables, with a value between 0 (no association) and 1 (perfect association).</p></div>
    <div id="cronbachs-alpha"><h4 class="font-semibold text-base">Cronbach's Alpha</h4><p>A measure of internal consistency or reliability, often used after a factor analysis. It assesses how well a set of questionnaire items collectively measure a single underlying concept (a factor).</p></div>
    <div id="eigenvalue"><h4 class="font-semibold text-base">Eigenvalue</h4><p>In factor analysis, an eigenvalue represents the amount of total variance explained by a single factor. It's used to help decide how many factors to retain in the analysis.</p></div>
    <div id="exploratory-factor-analysis"><h4 class="font-semibold text-base">Exploratory Factor Analysis (EFA)</h4><p>A data reduction technique used to identify underlying, unobservable variables (called factors or latent variables) from a larger set of observed, correlated variables. It's often used in questionnaire development.</p></div>
    <div id="factor"><h4 class="font-semibold text-base">Factor</h4><p>In factor analysis, a factor is an underlying, unobservable (latent) variable that is thought to influence several of the observed variables. For example, 'burnout' could be a factor measured by questions about exhaustion and cynicism.</p></div>
    <div id="factor-loading"><h4 class="font-semibold text-base">Factor Loading</h4><p>The correlation between an observed variable (e.g., a questionnaire item) and an underlying factor. A high loading suggests the item is a good measure of that factor.</p></div>
    <div id="kaplan-meier-curve"><h4 class="font-semibold text-base">Kaplan-Meier Curve</h4><p>A graph used in survival analysis that shows the probability of an event not occurring over time (e.g., the probability of surviving or remaining abstinent). Each drop in the line represents one or more participants experiencing the event.</p></div>
    <div id="kmo-test"><h4 class="font-semibold text-base">Kaiser-Meyer-Olkin (KMO) Test</h4><p>An assumption check for Exploratory Factor Analysis (EFA) that measures sampling adequacy. It tells you if your sample size is large enough and suitable for the analysis. A value above .7 is generally recommended.</p></div>
    <div id="latent-variable"><h4 class="font-semibold text-base">Latent Variable</h4><p>A variable or concept that cannot be directly observed but is inferred from other variables that are observed and measured. Examples include intelligence, burnout, or social learning.</p></div>
    <div id="log-rank-test"><h4 class="font-semibold text-base">Log-Rank Test</h4><p>A statistical test used in survival analysis to compare the survival distributions of two or more groups (e.g., comparing the Kaplan-Meier curves of a treatment group and a control group).</p></div>
    <div id="median-survival"><h4 class="font-semibold text-base">Median Survival</h4><p>In survival analysis, this is the point in time at which 50% of the participants in a group are expected to have experienced the event of interest.</p></div>
    <div id="moderator"><h4 class="font-semibold text-base">Moderator / Moderation</h4><p>A third variable that changes the strength or direction of the relationship between a predictor (IV) and an outcome (DV). Moderation answers the question: "When does X affect Y?"</p></div>
    <div id="nagelkerke-r-squared"><h4 class="font-semibold text-base">Nagelkerke RÂ²</h4><p>A "pseudo R-squared" value used in logistic regression. It provides an approximation of the proportion of variance in the outcome that is explained by the predictor variables, similar to RÂ² in linear regression.</p></div>
    <div id="nominal-data"><h4 class="font-semibold text-base">Nominal Data</h4><p>A type of categorical data where the categories have no natural order or ranking. For example, eye color (blue, brown, green).</p></div>
    <div id="odds"><h4 class="font-semibold text-base">Odds</h4><p>In logistic regression, the odds are the probability of an event occurring divided by the probability of it not occurring (p / (1-p)).</p></div>
    <div id="odds-ratio"><h4 class="font-semibold text-base">Odds Ratio (OR)</h4><p>A key output of logistic regression. It represents how the odds of an outcome change for a one-unit increase in a predictor variable. An OR greater than 1 means the odds increase; an OR less than 1 means the odds decrease.</p></div>
    <div id="ordinal-data"><h4 class="font-semibold text-base">Ordinal Data</h4><p>A type of categorical data where the categories have a natural order or ranking, but the differences between them are not necessarily equal. For example, degree classification (First, 2i, 2ii).</p></div>
    <div id="parallel-analysis"><h4 class="font-semibold text-base">Parallel Analysis</h4><p>A modern and accurate method used in factor analysis to determine how many factors to retain. It compares the eigenvalues from your actual data to eigenvalues from random data and keeps only the factors that explain more variance than chance.</p></div>
    <div id="reliability"><h4 class="font-semibold text-base">Reliability</h4><p>The consistency of a measure. In the context of questionnaires, it refers to whether the items that make up a scale are consistently measuring the same underlying construct. See Cronbach's Alpha.</p></div>
    <div id="sensitivity"><h4 class="font-semibold text-base">Sensitivity (True Positive Rate)</h4><p>In a classification model (like logistic regression), sensitivity measures how well the model correctly identifies the cases that experienced the outcome (e.g., how well it identifies students who actually passed).</p></div>
    <div id="simple-structure"><h4 class="font-semibold text-base">Simple Structure</h4><p>The ideal outcome in factor analysis, where each individual item loads strongly onto only one factor and has low loadings on all other factors. This makes the factor structure easy to interpret.</p></div>
    <div id="specificity"><h4 class="font-semibold text-base">Specificity (True Negative Rate)</h4><p>In a classification model (like logistic regression), specificity measures how well the model correctly identifies the cases that did not experience the outcome (e.g., how well it identifies students who actually failed).</p></div>
    <div id="standard-error"><h4 class="font-semibold text-base">Standard Error (SE / SEM)</h4><p>The standard error of the mean (SEM) is a measure of how precise your estimate of the sample mean is. A smaller SE means the sample mean is likely to be a more accurate reflection of the true population mean. It is often shown as error bars on graphs.</p></div>
    <div id="standardized-score"><h4 class="font-semibold text-base">Standardized Score (Z-score)</h4><p>A score that has been converted into a universal unit: the number of standard deviations it is from its own mean. This allows for fair comparison of scores from different scales.</p></div>
    <div id="survival-analysis"><h4 class="font-semibold text-base">Survival Analysis</h4><p>A set of statistical methods used to analyze longitudinal data on the time it takes for a specific event to occur (e.g., recovery, relapse, death).</p></div>
    <div id="unstandardized-coefficient"><h4 class="font-semibold text-base">Unstandardized Coefficient (B)</h4><p>In regression, this is another name for the slope. It represents the change in the outcome variable for a one-unit increase in the predictor variable, expressed in the original units of measurement.</p></div>
    <div id="welchs-f-test"><h4 class="font-semibold text-base">Welch's F-test</h4><p>A more robust version of the F-test in an independent ANOVA that should be reported if the assumption of homogeneity of variance is violated (i.e., if Levene's test is significant).</p></div>
</div>
